{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAT5243 Project 4: Causal Inference Algorithms Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Group 2:***\n",
    "\n",
    "    Wendy Doan (ad3801)\n",
    "    Yibai Liu (yl4616)\n",
    "    Shuqi Yu (sy2950)\n",
    "    Yiwen Fang (yf2560)\n",
    "    Zhihang Xia (zx2338)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we studied **Causal Inference** on two datasets, one high-dimensional and another low-dimensional. \n",
    "\n",
    "Specifically, we estimated the **Average Treatment Effects (ATE)** by calculating the **Propensity Scores (PS)** using the **Boosting Stumps** algorithm. \n",
    "\n",
    "The ATEs are then calculated using three models and compared with the true values to estimate accuracy. For each algorithm and method,  The performance and computational efficiency were evaluated for each dataset to select the best combination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propensity Scores: (Boosting Stumps)\n",
    "1. GBM \n",
    "2. XGboost\n",
    "\n",
    "ATE Estimation Methods:\n",
    "1. Stratification\n",
    "2. Regression Adjustment\n",
    "3. Stratification + Regression Adjustment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aid with propensity score prediction, we also attempted to alleviate the slight imbalance in the data through well known methods such as random oversampling and SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity Score Estimation\n",
    "\n",
    "We define the propensity score as:\n",
    "\n",
    "$$e(x) = Pr(T =1|X=x)$$\n",
    "\n",
    "The propensity score is given in term of probability \n",
    "\n",
    "$$0<e(x)<1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 0: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set a random seed for reproduction.\n",
    "RANDOM_STATE = np.random.seed(42)\n",
    "\n",
    "# train-test split for propensity score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# baseline propensity score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# propensity score from tree models\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# GBM\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Stratification and Regression Adjustment\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Imbalance techniques\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import and explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase the usability of the notebook, we use generic variable names instead of associating with the datasets used in for the project. To use the notebook with other datasets, simply import with variable name `dataset`\n",
    "\n",
    "We have two dataset, `High Dimensional Dataset` and `Low Dimensional Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your directory for the datasets\n",
    "directory = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../data/highDim_dataset.csv does not exist: '../data/highDim_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-506b63654ec9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhighDim_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'data/highDim_dataset.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlowDim_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'data/lowDim_dataset.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File ../data/highDim_dataset.csv does not exist: '../data/highDim_dataset.csv'"
     ]
    }
   ],
   "source": [
    "highDim_dataset = pd.read_csv(directory + 'data/highDim_dataset.csv')\n",
    "lowDim_dataset = pd.read_csv(directory + 'data/lowDim_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is estimating the ATE of two dataset: high and low dimensional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_true_ATE = -54.8558\n",
    "low_true_ATE = 2.0901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "# Choose a development option between highDim_dataset and lowDim_dataset\n",
    "#========================================================================\n",
    "\n",
    "dataset = lowDim_dataset.copy()\n",
    "dataset_name = \"Low-Dimensional Dataset\"\n",
    "true_ATE = low_true_ATE\n",
    "\n",
    "#dataset = highDim_dataset.copy()\n",
    "#dataset_name = \"High-Dimensional Dataset\"\n",
    "#true_ATE = high_true_ATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high-dimensional dataset has 2000 observations of 187 dimensions, while the low-dimensional dataset has 500 observations of 24 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to extract only the X portion, which is columns other than Y (treatment result) and A (binary treatment/control group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = dataset.drop(['Y', 'A'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotComposition(dataset, reset_index=False):\n",
    "    plt.plot(dataset[dataset.A == 1].Y.reset_index(drop=True) \n",
    "             if reset_index \n",
    "             else dataset[dataset.A == 1].Y, 'o', label='Treatment')\n",
    "    plt.plot(dataset[dataset.A == 0].Y.reset_index(drop=True) \n",
    "             if reset_index \n",
    "             else dataset[dataset.A == 0].Y, 'ro', label='Control')\n",
    "    plt.title(dataset_name + \" Distribution\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotComposition(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Naive estimate of ATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the origianl ATE for both high and low dimension data without any steps and algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_ATE(dataset):\n",
    "    return np.average(dataset[dataset.A == 1].Y) - np.average(dataset[dataset.A == 0].Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive ATE for high-dimensional data:\", naive_ATE(highDim_dataset))\n",
    "print(\"Naive ATE for low-dimensional data:\", naive_ATE(lowDim_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the data sets balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkComposition(dataset):\n",
    "    print(\"The dataset contains:\\n\",len(dataset[dataset.A == 1]), \"cases in Treatment group\\n\", \n",
    "          len(dataset[dataset.A == 0]), \"cases in Control group.\")\n",
    "    print(\"Treatment/Control ratio: {}/100\".format(round(len(dataset[dataset.A == 1])/len(dataset[dataset.A == 0])*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"High-dimensional\")\n",
    "checkComposition(highDim_dataset)\n",
    "print('-'*20)\n",
    "print(\"Low-dimensional\")\n",
    "checkComposition(lowDim_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the high-dimensional data is slightly imbalanced, but acceptable. However, the low-dimensional data displays severer imbalance between groups. In any cases, one can use oversampling or smote to balance the data, which however may not be beneficial for some ATE estimation algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling to deal with the imbalanced data\n",
    "\n",
    "Resampling data is one of the most commonly preferred approaches to deal with an imbalanced dataset. We used oversampling the minority instead of undersampling the majority since undersampling removes instances from data that may be carrying important information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Random Oversampling**: \n",
    "\n",
    "To randomly replicate the small sample to match the size of the larger sample.\n",
    "\n",
    "2. **SMOTE**: Synthetic Minority Oversampling Technique\n",
    "\n",
    "SMOTE generates synthetic samples from the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling. It focuses on the feature space to generate new instances with the help of interpolation between the positive instances that lie together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Oversample(dataset):\n",
    "    ovs = RandomOverSampler(random_state = RANDOM_STATE)\n",
    "    x, y = ovs.fit_resample(dataset.loc[:, dataset.columns != 'A'], dataset.A)\n",
    "    x.insert(1, 'A', y)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_oversampled = Oversample(dataset)\n",
    "\n",
    "checkComposition(dataset_oversampled)\n",
    "plotComposition(dataset_oversampled, reset_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMOTE(dataset):\n",
    "    smk = SMOTETomek()\n",
    "    x, y = smk.fit_resample(dataset.loc[:, dataset.columns != 'A'], dataset.A)\n",
    "    x.insert(1, 'A', y)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_smote = SMOTE(dataset)\n",
    "\n",
    "checkComposition(dataset_smote)\n",
    "plotComposition(dataset_smote, reset_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, there are three dataset that can be used: orginial, oversampled, and SMOTE. For testing purpose, change the variable splitted in the beginning of the next section, instead of pasting them all into one notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Calculating the propensity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The propensity score is estimated by applying machine learning methods on the `X` variable to fit the label `A`. For this purpose (and this purpose only) the dataset is splitted into train and test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into train and test with the porpotion of 20:80. We will use the train data to train the model and do the cross validation to avoid overfitting. Then use the test data to check our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(dataset, test_size=0.2):\n",
    "    X = np.array(dataset.drop(['Y', 'A'], axis=1))\n",
    "    y = np.array(dataset.Y)\n",
    "    A = np.array(dataset.A)\n",
    "    \n",
    "    idx_train, idx_test, y_train, y_test = train_test_split(range(X.shape[0]), y, \n",
    "                                                test_size=0.2, random_state=RANDOM_STATE)\n",
    "    X_train = X[idx_train]\n",
    "    X_test = X[idx_test]\n",
    "    A_train = A[idx_train]\n",
    "    A_test = A[idx_test]\n",
    "    return X_train, X_test, y_train, y_test, A_train, A_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update dataset name HERE to test the original, oversampled, SMOTE, or any new dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================\n",
    "# Development choices include dataset, dataset_oversampled and dataset_smote\n",
    "#=============================================================================\n",
    "\n",
    "X_train, X_test, y_train, y_test, A_train, A_test = split_train_test(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propensity Score - Boosted Stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we use GBM and XGboost. For any choices of learner, it is desirable to perform cross validation and grid search for the best model. We then evaluate the model prediction on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "              'learning_rate': [0.1, 0.05, 0.01],\n",
    "              'max_depth': [2, 3, 5],\n",
    "              'min_samples_split': [2, 4],\n",
    "              'n_estimators': [5, 10, 15, 20],\n",
    "              'min_samples_leaf':[1, 3, 5]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X:np.array, A:np.array, model, param_grid=param_grid, cv=10, print_step=True, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Takes a baseline model and does grid search among parameters in the param_grid with cross validation.\n",
    "    Returns the model with best hyparameters after searching\n",
    "    \"\"\"\n",
    "    if sample_weight is None:\n",
    "        clf = GridSearchCV(model, param_grid, cv=cv, n_jobs=-1, scoring = 'roc_auc').fit(X, A)\n",
    "    else:\n",
    "        clf = GridSearchCV(model, param_grid, cv=cv, n_jobs=-1, scoring = 'roc_auc').fit(X, A, \n",
    "                                                                sample_weight = sample_weight)\n",
    "    print(\"Best accuracy: %0.3f\" % (clf.best_score_))\n",
    "    print()\n",
    "    print(\"Best parameters: %r\" % clf.best_params_)\n",
    "    print('-'*30)\n",
    "    if print_step:\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "        print('-'*30)\n",
    "\n",
    "    return(model.set_params(**clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cross validation takes a few minutes. Uncomment this cell for developement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbm = grid_search(\n",
    "#     X_train, A_train, model=GradientBoostingClassifier(random_state=RANDOM_STATE), \n",
    "#     param_grid=param_grid,\n",
    "#     print_step=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A copy of best hyperparameters from grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 3, \n",
    "               'min_samples_split': 4, 'n_estimators': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingClassifier(random_state=RANDOM_STATE).set_params(**best_params).fit(X_train, A_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbm.score(X_train, A_train))\n",
    "print(gbm.score(X_test, A_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict propensity scores\n",
    "propensity_score_gbm = np.exp(gbm.predict_log_proba(dataset.iloc[:, 2:]))[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_xgb = {\n",
    "    'objective':['binary:logistic', 'reg:squarederror'],\n",
    "    'n_estimators':[5, 10, 15, 20],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'gamma': [0.5, 1.0, 1.5],\n",
    "    'subsample': [0.3, 0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8],\n",
    "    'max_depth': [3, 4]\n",
    "    #'scale_pos_weight':[1, 2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cross validation takes a few minutes. Uncomment this cell for developement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb = grid_search(X_train, A_train, \n",
    "#             model=XGBClassifier(n_jobs=-1,random_state=RANDOM_STATE),\n",
    "#             param_grid = param_grid_xgb, \n",
    "#             print_step=True, \n",
    "#             cv=5\n",
    "#            )           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A copy of best hyperparameters from grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_xgb = {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 4, \n",
    "                   'min_child_weight': 1, 'n_estimators': 20,     \n",
    "                   'objective': 'reg:squarederror', 'subsample': 1.0}\n",
    "\n",
    "xgb = XGBClassifier(n_jobs=-1,random_state=RANDOM_STATE).set_params(**best_params_xgb).fit(X_train, A_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb.score(X_train, A_train))\n",
    "print(xgb.score(X_test, A_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict propensity scores\n",
    "propensity_score_xgb = xgb.predict_proba(np.array(dataset.iloc[:, 2:]))[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we have obtained the propensity scores using GBM and XGBoost. These can now be used to calculate the ATE score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Extract relevant data for ATE calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to attach the newly constructed propensity score with `A` and `Y`. For the stratification, this is all required. However for regression method, `X` is also required to remove further confounding factors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(ps, A, y):\n",
    "    \"\"\"\n",
    "    Combines propensity scores with A and y\n",
    "    \n",
    "    Returns a data frame with three columns\n",
    "    \"\"\"\n",
    "    data_combined = pd.DataFrame(\n",
    "        np.array([ps, A, y]).T,\n",
    "        columns=['e', 'A', 'Y']\n",
    "    )\n",
    "    return data_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We summarized the selected models in the following pipeline:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation_pipeline(dataset:pd.DataFrame, resample=None, label=dataset_name, boost='GBM'):\n",
    "    \"\"\"\n",
    "    Each sampling method results in a different data size, a set of best \n",
    "    hyperparameters from grid search, and an array of sample weights. \n",
    "    This pipeline prepares a dataset, with the specified resampling method, \n",
    "    for the later ATE estimation.\n",
    "    \n",
    "    inputs\n",
    "    -------\n",
    "    dataset: pd.DataFrame, the dataset used to evaluate algorithms\n",
    "    resample: str or None, used when resampling methods are applied. Possible values are None, 'over', or 'smote'\n",
    "    label: str, name of the dataset\n",
    "    boost: str, the boosting method used to predict propensity scores. Possible values are 'GBM', 'XGB'\n",
    "    \n",
    "    outputs\n",
    "    -------\n",
    "    ps_data: pd.DataFrame, contains three columns for propensity scores, group, and outcome variable\n",
    "    X_data: pd.DataFrame, contains X variables\n",
    "    \"\"\"\n",
    "    test_scores = []\n",
    "    params = []\n",
    "    weights =[]\n",
    "\n",
    "    if resample == None:\n",
    "        data = dataset\n",
    "    elif resample == 'over':\n",
    "        data = Oversample(dataset)\n",
    "    elif resample == 'smote':\n",
    "        data = SMOTE(dataset)\n",
    "    else:\n",
    "        print(\"Error: Invalid resampling method! Possible options include None, 'over' and 'smote'\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, A_train, A_test = split_train_test(data)\n",
    "    \n",
    "    # ================================== Predict PS with GBM ===================================\n",
    "    if boost=='GBM':    \n",
    "        # A copy of the best hyperparameter candidates from grid search:\n",
    "        if label == 'Low-Dimensional Dataset':\n",
    "            gbm_params1 =  {'learning_rate': 0.05, 'max_depth': 2, 'min_samples_leaf': 3, \n",
    "                            'min_samples_split': 2,'n_estimators': 100}\n",
    "            gbm_params2 = {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 1, \n",
    "                           'min_samples_split': 4,'n_estimators': 100}\n",
    "        elif label == \"High-Dimensional Dataset\":\n",
    "            gbm_params1 =  {'learning_rate': 0.05, 'max_depth': 2, 'min_samples_leaf': 3, \n",
    "                            'min_samples_split': 2, 'n_estimators': 100}\n",
    "            gbm_params2 = {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 1, \n",
    "                           'min_samples_split': 4, 'n_estimators': 100}\n",
    "        else:\n",
    "            print(\"Error: Invalid resampling method! Possible options include None, 'over' and 'smote'\")\n",
    "            \n",
    "        params_list = [gbm_params1, gbm_params2]\n",
    "        \n",
    "        for i in range(15,22):\n",
    "            # high-dimensional weights (the best weights after multiple trials)\n",
    "            sample_weights = np.zeros(len(A_train))\n",
    "            sample_weights[A_train == 0] = i\n",
    "            sample_weights[A_train == 1] = 20\n",
    "            \n",
    "            for p in params_list:\n",
    "                gbm = GradientBoostingClassifier().set_params(**p).fit(X_train, A_train,sample_weight=sample_weights)\n",
    "                #print(gbm.score(X_train, A_train), gbm.score(X_test, A_test))\n",
    "                test_scores.append(gbm.score(X_test, A_test))\n",
    "                params.append(p)\n",
    "                weights.append(sample_weights)\n",
    "            \n",
    "        best_ = params[test_scores.index(max(test_scores))]\n",
    "        print(\"GBM parameters:\", best_)\n",
    "        gbm = GradientBoostingClassifier().set_params(**best_).fit(X_train, A_train,\n",
    "                                  sample_weight=weights[test_scores.index(max(test_scores))])\n",
    "        print(\"GBM train accuracy: \",gbm.score(X_train, A_train))\n",
    "        print(\"GBM test accuracy: \", gbm.score(X_test, A_test))\n",
    "        propensity_score_gbm = np.exp(gbm.predict_log_proba(data.iloc[:, 2:]))[:, 1]\n",
    "    \n",
    "        ps_data = combine_data(propensity_score_gbm, data.A, data.Y)\n",
    "    \n",
    "    # ================================== Predict PS with XGB ==================================\n",
    "    elif boost=='XGB':\n",
    "        best_params_xgb = {'colsample_bytree': 0.8, 'gamma': 0.5, 'max_depth': 4, \n",
    "                   'min_child_weight': 1, 'n_estimators': 20,     \n",
    "                   'objective': 'reg:squarederror', 'subsample': 1.0}\n",
    "\n",
    "        xgb = XGBClassifier(n_jobs=-1,random_state=RANDOM_STATE).set_params(**best_params_xgb).fit(X_train, A_train)\n",
    "        print(\"XGboost train accuracy: \",xgb.score(X_train, A_train))\n",
    "        print(\"XGboost test accuracy: \", xgb.score(X_test, A_test))\n",
    "        propensity_score_xgb = xgb.predict_proba(np.array(data.iloc[:, 2:]))[:, 1]\n",
    "        ps_data = combine_data(propensity_score_xgb, data.A, data.Y)\n",
    "        \n",
    "    else:\n",
    "        print(\"Error: Wrong boosting methods! Possible options include 'GBM' and 'XGB'\")\n",
    "        \n",
    "    X_data = data.drop(['Y', 'A'], axis=1)\n",
    "    return ps_data, X_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Calculating ATE with different algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATE Estimate - Stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common approach to estimate ATE using stratification based on propensity scores. The procedure is as follow: : (i) Estimate propensity scores $e_i$ accross all samples; (ii) form $K$ strata according to the sample quantiles of the $e_i$, such that the treated and control have roughly the same proportion within each strata; (iii) within each stratum, calculate the difference of sample means of the $Y_i$ for each treatment; and (iv) estimate $\\Delta$ by a weighted sum of the differences of sample means across strata, where weighting is by the proportion of observations falling in each stratum\n",
    "\n",
    "$$ \n",
    "\\hat{∆}_S = \\sum_{j=1}^{k}(N_j / N)\\{N^{−1}_{1j} \\sum_{i=1}^{N} T_i*Y_i*I(e_i ∈ Q_j ) − N^{−1}_{0j}*\\sum^{N}_{i=1}(1 − T_i)Y_i*I(e_i ∈ Q_j ) \n",
    "$$\n",
    "\n",
    "where K is the number of strata, some literature have advocate to use quintiles (K=5). \n",
    "$N_{j}$ is the number of individuals in stratum j. $N_{1j}$ is the number of “treated” individuals in stratum `j`, while $N_{0j}$ is the number of “controlled” individuals in stratum `j`. $$Q_j = (q_{j−1}, q_{j}] $$ where $q_j$ is the jth sample quantile of the estimated\n",
    "propensity scores. (See Lunceford and Davidian (2004))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stratify(ps_data:pd.DataFrame, k:int):\n",
    "    \n",
    "    data_copy = ps_data.copy()\n",
    "    \n",
    "    # rank to resolve duplicate edge cases\n",
    "    data_copy['bin'] = pd.qcut(\n",
    "        data_copy.e.rank(method='first'), k, labels=False\n",
    "    )\n",
    "    \n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_ATE_Strat(ps_data:pd.DataFrame, k:int):\n",
    "    \n",
    "    n = ps_data.shape[0]\n",
    "    \n",
    "    data_copy = Stratify(ps_data, k)\n",
    "    \n",
    "    # calculate ATE score\n",
    "    ATE = 0\n",
    "    for k_idx in range(k):\n",
    "        \n",
    "        # temporary data frame\n",
    "        Qj = data_copy[data_copy.bin == k_idx]\n",
    "        nj = Qj.shape[0]\n",
    "\n",
    "        treat_avg = np.average(Qj[Qj.A==1].Y) if Qj[Qj.A==1].shape[0] != 0 else 0\n",
    "        control_avg = np.average(Qj[Qj.A==0].Y) if Qj[Qj.A==0].shape[0] != 0 else 0\n",
    "        ATE += (nj/n) * ( treat_avg - control_avg )\n",
    "    \n",
    "    return ATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATE Estimate - Regression and Stratification + Regression Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression adjustment can be employed to reduce residual within-stratum confounding. With regression adjustment, data for each bin is further corrected using regression on `X`, with level variable `A`. \n",
    "\n",
    "Here, steps (iii) and (iv) above are modified as follows: (iii) within each stratum $j = 1,...,K,$ fit a regression model of the form $m^{(j)}(T, X, \\alpha(j))$ representing the postulated regression relationship $E(Y |T, X)$ within stratum $j$ and, based on the result, estimate treatment effect in stratum $j$ by averaging over $X_i$ in $j$ as\n",
    "\n",
    "$$ \n",
    "\\hat{\\Delta}^{(j)} = n^{-1}_{j} \\sum_{i = 1}^{n} I(e_i ∈ Q_j ) (\n",
    "m^{(j)}(1, X, \\alpha^{(j)}) - m^{(j)}(0, X, \\alpha^{(j)}) )\n",
    "$$\n",
    "\n",
    "and (iv) estimate $\\Delta$ by the averaging\n",
    "\n",
    "$$\n",
    "\\hat{\\Delta}_{SR} = \\frac{1}{K} \\sum_{j=1}^{K} \\Delta^{(j)}\n",
    "$$\n",
    "\n",
    "Note that a variation here is using two separate regression for $T$, which is not considered here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, setting k = 1 bin is equivalent to performing only regression estimation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_ATE_StratRegrAdjusted_with_X(data:pd.DataFrame, X_data, k:int):\n",
    "    \n",
    "    n = data.shape[0]\n",
    "    \n",
    "    data_adjusted = pd.concat(\n",
    "        [Stratify(data, k), X_data.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # calculate ATE score\n",
    "    ATE = 0\n",
    "    for k_idx in range(k):\n",
    "        \n",
    "        # temporary data frame\n",
    "        Qj = data_adjusted[data_adjusted.bin == k_idx]\n",
    "        nj = Qj.shape[0]\n",
    " \n",
    "        # Regression Adjusted Linearly, then Delta_j = alpha^Z_j\n",
    "        X = Qj.drop(['e', 'Y', 'bin'], axis=1)\n",
    "        y = Qj.Y\n",
    "        reg = LinearRegression().fit(X, y)\n",
    "        \n",
    "        ATE += reg.coef_[0]\n",
    "        \n",
    "    return ATE / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the uncertainty of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the accuracy, we used the squared error\n",
    "\n",
    "$$\n",
    "SE = (\\hat{\\Delta} - \\Delta )^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the true error is often not known, one can alternatively estimate the uncertainty by random sampling the dataset. To give an example, here is an estimate for the $\\hat{\\Delta}_S$ using default GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_ATE_Strat_CV(\n",
    "    data_for_ATE:pd.DataFrame, k:int, frac=0.9, n=500\n",
    "):\n",
    "    ATEs = []\n",
    "    m = data_for_ATE.shape[0]\n",
    "    for i in range(n):\n",
    "        temp = data_for_ATE.sample(frac=frac, replace=True)\n",
    "        ATEs.append( Calculate_ATE_Strat(temp, k) )\n",
    "        \n",
    "    return ATEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_ATE = combine_data(propensity_score_gbm, dataset.A, dataset.Y)\n",
    "ATEs = Calculate_ATE_Strat_CV(data_for_ATE, 5)\n",
    "print(\"Mean: \", np.nanmean(ATEs), \"\\n\" + \"Stdev:\", np.nanstd(ATEs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 9\n",
    "n, bins, patches = plt.hist(ATEs, num_bins, facecolor='pink', alpha=0.5)\n",
    "plt.title(dataset_name + \" Distribution using default GBM and and Stratification\")\n",
    "plt.xlabel(\"ATE estimate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for high dimensional dataset, the result can be trusted with uncertainty at around $5 \\%$, while for low dimensional dataset only give order of magnitude estimate. This may be due the to high dimensional dataset being larger. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cols=[\"Model\", \"Data\", \"PS model\", \"Estimation time(s)\",\"Squared error\"]\n",
    "summary = pd.DataFrame(columns=summary_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_summary(dataset:pd.DataFrame, max_k:int, true_ATE=true_ATE, title=\"high\", resample=None, boost='GBM'):\n",
    "    \"\"\"\n",
    "    This function reads in data and returns best ATE estimation generated from the \n",
    "    most appropriate k value for each algorithm. This function is \n",
    "    \n",
    "    inputs\n",
    "    -------\n",
    "    dataset: pd.DataFrame, the dataset used to evaluate algorithms\n",
    "    max_k: int, the maximum k value used to calculate ATE\n",
    "    true_ATE: float, the true ATE score used to plot against estimations\n",
    "    title: str, a string used to label high/low-dimensional datasets in the plot. Possible values are \"high\" or \"low\"\n",
    "    resample: str or None, used when resampling methods are applied. Possible values are None, 'over', or 'smote'\n",
    "    boost: str, the boosting method used to predict propensity scores. Possible values are 'GBM', 'XGB'\n",
    "    \n",
    "    outputs\n",
    "    -------\n",
    "    log: pd.DataFrame, chunk of summary table generated from running the experiment\n",
    "    \n",
    "    \"\"\"\n",
    "    data, data_X = data_preparation_pipeline(dataset, resample=resample, label=dataset_name, boost=boost)\n",
    "    \n",
    "    strat_results = [Calculate_ATE_Strat(data, i) for i in range(1, max_k+1)]\n",
    "    strat_reg_results = [Calculate_ATE_StratRegrAdjusted_with_X(data, data_X, i) for i in range(1, max_k+1)]\n",
    "    \n",
    "    error = [abs(strat_results[i] - true_ATE) for i in range(len(strat_results))]\n",
    "    best_k_strat = error.index(min(error))+1\n",
    "    #best_ATE_strat = strat_results[best_k_strat]\n",
    "    \n",
    "    error = [abs(strat_reg_results[i] - true_ATE) for i in range(len(strat_reg_results))]\n",
    "    best_k_strat_reg = error.index(min(error))+1\n",
    "    #best_ATE_strat_reg = strat_reg_results[best_k_strat_reg]\n",
    "    \n",
    "    start = time.time()\n",
    "    strat = Calculate_ATE_Strat(data, best_k_strat)\n",
    "    t_strat = time.time()-start\n",
    "    print(\"Time for calculating ATE with stratification: {}s\".format(round(t_strat, 4)))\n",
    "    \n",
    "    start = time.time()\n",
    "    regadj = Calculate_ATE_StratRegrAdjusted_with_X(data, data_X, 1)\n",
    "    t_regadj = time.time()-start\n",
    "    print(\"Time for calculating ATE with regression adjustment: {}s\".format(round(t_regadj, 4)))\n",
    "    \n",
    "    start = time.time()\n",
    "    combined = Calculate_ATE_StratRegrAdjusted_with_X(data, data_X, best_k_strat_reg)\n",
    "    t_combined = time.time()-start\n",
    "    print(\"Time for calculating ATE with stratification + regression adjustment: {}s\".format(round(t_combined, 4)))\n",
    "    \n",
    "    print(\"Estimated ATE by stratification with k = {}: {}\".format(best_k_strat, strat))\n",
    "    print(\"Estimated ATE by regression adjustment: {}\".format(regadj))\n",
    "    print(\"Estimated ATE by stratification + regression adjustment with k = {}: {}\".format(best_k_strat_reg, combined))\n",
    "    \n",
    "    # resamping label\n",
    "    if resample == 'over':\n",
    "        resample_str = '(oversampled)'\n",
    "    elif resample == 'smote':\n",
    "        resample_str = '(SMOTE)'\n",
    "    else: resample_str = ''\n",
    "    \n",
    "    # Add results to summary\n",
    "    log = pd.DataFrame(columns=summary_cols)\n",
    "    row1= pd.DataFrame([[str('Stratification (K={})'.format(best_k_strat+1)), str(title+'-dim'+resample_str), boost, \n",
    "                         t_strat, round((strat-true_ATE)**2,4)]], columns = summary_cols)\n",
    "    log = log.append(row1)\n",
    "    row2= pd.DataFrame([['Regression Adjustment', str(title+'-dim'+resample_str), boost, \n",
    "                         t_regadj, round((regadj - true_ATE)**2,4)]], columns = summary_cols)\n",
    "    log = log.append(row2)\n",
    "    row3= pd.DataFrame([[str('Strat. + Reg. Adj. (K={})'.format(best_k_strat_reg+1)), str(title+'-dim'+resample_str), boost, \n",
    "                         t_combined, round((combined - true_ATE)**2,4)]], columns = summary_cols)\n",
    "    log = log.append(row3)\n",
    "    \n",
    "    \n",
    "    # Plot: \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(range(1,max_k+1), strat_results, marker='o',\n",
    "                label = 'ATE estimate by stratification')\n",
    "    plt.plot(range(1,max_k+1), strat_reg_results, marker='o',\n",
    "                label = 'ATE estimate by strat + reg_adj')\n",
    "    plt.scatter(1, regadj, s=150, label='ATE estimate by regression adjustment',alpha=1, marker='o',c='green')\n",
    "\n",
    "    plt.hlines(true_ATE, 1, max_k, colors='red', linestyles='dashed', label='ATE true')\n",
    "    #plt.hlines(naive_ATE(data), 1, 10, colors='grey', linestyles='dashed', label='ATE Naive')\n",
    "    plt.title(\"The {}-dimensional dataset {}\\nTrue vs. Estimated ATE (PS predicted by {})\".format(title, resample_str,boost))\n",
    "    plt.xlabel(\"Number of strata (k)\")\n",
    "    plt.ylabel(\"ATE\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize the stability of stratification and strat + reg.adj. for different K values:\n",
    "# Development choice: use the lowDim dataset\n",
    "dataset = lowDim_dataset.copy()\n",
    "dataset_name = \"Low-Dimensional Dataset\"\n",
    "true_ATE = low_true_ATE\n",
    "\n",
    "for r in [None,'over','smote']:\n",
    "    for b in ['GBM','XGB']:\n",
    "        log = detailed_summary(dataset, max_k=10, true_ATE=true_ATE, title=\"low\", resample=r, boost=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimation_summary(dataset:pd.DataFrame, k:int, true_ATE=true_ATE, title=\"high\", resample=None, boost='GBM'):\n",
    "    \"\"\"\n",
    "    This function reads in data and returns best ATE estimation generated from each algorithm.\n",
    "    \n",
    "    inputs\n",
    "    -------\n",
    "    dataset: pd.DataFrame, the dataset used to evaluate algorithms\n",
    "    k: int, the k value used in stratification to calculate ATE \n",
    "    true_ATE: float, the true ATE score used to plot against estimations\n",
    "    title: str, a string used to label high/low-dimensional datasets in the plot. Possible values are \"high\" or \"low\"\n",
    "    resample: str or None, used when resampling methods are applied. Possible values are None, 'over', or 'smote'\n",
    "    boost: str, the boosting method used to predict propensity scores. Possible values are 'GBM', 'XGB'\n",
    "    \n",
    "    outputs\n",
    "    -------\n",
    "    log: pd.DataFrame, chunk of summary table generated from running the experiment\n",
    "    \n",
    "    \"\"\"\n",
    "    data, data_X = data_preparation_pipeline(dataset, resample=resample, label=dataset_name, boost=boost)\n",
    "    \n",
    "    start = time.time()\n",
    "    strat = Calculate_ATE_Strat(data, k)\n",
    "    t_strat = time.time()-start\n",
    "    print(\"Time for calculating ATE with stratification: {}s\".format(round(t_strat, 4)))\n",
    "    \n",
    "    start = time.time()\n",
    "    regadj = Calculate_ATE_StratRegrAdjusted_with_X(data, data_X, 1)\n",
    "    t_regadj = time.time()-start\n",
    "    print(\"Time for calculating ATE with regression adjustment: {}s\".format(round(t_regadj, 4)))\n",
    "    \n",
    "    start = time.time()\n",
    "    combined = Calculate_ATE_StratRegrAdjusted_with_X(data, data_X, k)\n",
    "    t_combined = time.time()-start\n",
    "    print(\"Time for calculating ATE with stratification + regression adjustment: {}s\".format(round(t_combined, 4)))\n",
    "    \n",
    "    print(\"Estimated ATE by stratification with k = {}: {}\".format(k, strat))\n",
    "    print(\"Estimated ATE by regression adjustment: {}\".format(regadj))\n",
    "    print(\"Estimated ATE by stratification + regression adjustment with k = {}: {}\".format(k, combined))\n",
    "    \n",
    "    # resamping label\n",
    "    if resample == 'over':\n",
    "        resample_str = '(oversampled)'\n",
    "    elif resample == 'smote':\n",
    "        resample_str = '(SMOTE)'\n",
    "    else: resample_str = ''\n",
    "    \n",
    "    # Add results to summary\n",
    "    log = pd.DataFrame(columns=summary_cols)\n",
    "    row1= pd.DataFrame([[str('Stratification (K={})'.format(k)), str(title+'-dim'+resample_str), boost, \n",
    "                         t_strat, round((strat-true_ATE)**2,4)]], columns = summary_cols)\n",
    "    log = log.append(row1)\n",
    "    row2= pd.DataFrame([['Regression Adjustment', str(title+'-dim'+resample_str), boost, \n",
    "                         t_regadj, round((regadj - true_ATE)**2,4)]], columns = summary_cols)\n",
    "    log = log.append(row2)\n",
    "    row3= pd.DataFrame([[str('Strat. + Reg. Adj. (K={})'.format(k)), str(title+'-dim'+resample_str), boost, \n",
    "                         t_combined, round((combined - true_ATE)**2,4)]], columns = summary_cols)\n",
    "    log = log.append(row3)\n",
    "\n",
    "    return log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High dimensional dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = highDim_dataset.copy()\n",
    "dataset_name = \"High-Dimensional Dataset\"\n",
    "true_ATE = high_true_ATE\n",
    "\n",
    "for r in [None,'over','smote']:\n",
    "    for b in ['GBM','XGB']:\n",
    "        log = estimation_summary(dataset, k=5, true_ATE=true_ATE, title=\"high\", resample=r, boost=b)\n",
    "        summary = summary.append(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The ATE estimates become more inaccurate for larger numbers of strata (K>8), so here we only plot reasonable K values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low dimensional dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = lowDim_dataset.copy()\n",
    "dataset_name = \"Low-Dimensional Dataset\"\n",
    "true_ATE = low_true_ATE\n",
    "\n",
    "for r in [None,'over','smote']:\n",
    "    for b in ['GBM','XGB']:\n",
    "        log = estimation_summary(dataset, k=5, true_ATE=true_ATE, title=\"low\", resample=r, boost=b)\n",
    "        summary = summary.append(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summary.reset_index().drop(['index'], axis=1)\n",
    "summary.to_csv('../output/evaluations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and recommendations\n",
    "\n",
    "Among three algorithms, **regression** is the simplest, while **stratification with regression adjustment** is the most complicated model. The regression precedure makes use of the covariate variable $X$ to account for the confounding factor, while the **stratification** makes use of the assumption that the treatment exposure is unrelated to the counterfactuals for individual sharing the propensity score (X and T independent within strata). Theoretically, the **stratification with regression adjustment**, which was shown to offer an unbiased estimate, offers a **double robustness** even when the regression models are incorrect, thus should be the most accurate.  \n",
    "\n",
    "On the estimation accuracy, for the low-dimensional dataset, **stratification with regression adjustment** has the most accurate estimate. On the high-dimensional dataset, **stratification** achieves the best ATE estimate closest to the true ATE by setting $k = 5$. Interestingly, the **regression** gives a smaller estimate, while **regression adjustment to stratification** gives higher estimate. This reduction of accuracy may be due to esimation for the propensity score being overfitted. A shallower tree stump model (with only 10 estimator) indicates the reverse: **stratification adjustment** actually offers an improvement over **stratification** and gives the best result. \n",
    "\n",
    "The number of strata $k$ was optimized at 5 bins, as higher number of bins creates imbalance between strata. For some higher number of $k$, it was observed that some bins contain no $T = 1$ datapoint.\n",
    "\n",
    "On time complexity, **stratification with regression adjustment** is generally the lowest when estimating ATE. **Regression adjustment** works the fastest when the dataset has low dimensions, but when the dataset increases in its dimensionality, **Regression adjustment** slows down and **stratification** becomes faster.\n",
    "\n",
    "We also discussed the effects of resampling on the model performance. Since the dataset was not too imbalanced and GBM model is quite robust dealing with data imbalance, the resampling methods didn't bring much improvement. \n",
    "\n",
    "In summary, both **stratification** and **stratification with regression adjustment** are flexible which enable us to choose a specified k that fits the dataset better, whereas **regression adjustment** procedure is mostly set and offers less adjustibility. To check the **stratification**, it is recommended that one makes sure the bins are balanced between the treatment/control group. Finally, the propensity score should not be overfitted, as this will skew the confound factor extracted from the covariate variable $X$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.read_csv('../output/evaluation.csv')\n",
    "# Sort by estimation accuracy\n",
    "evaluation.sort_values(by=['Squared error', 'Estimation time(s)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by time complexity\n",
    "evaluation.sort_values(by=['Estimation time(s)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\n",
    "1. Chan, D., Ge, R., Gershony, O., Hesterberg, T. and Lambert, D., 2010, July. Evaluating online ad campaigns in a pipeline: causal models at scale. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 7-16). https://doi.org/10.1145/1835804.1835809\n",
    "\n",
    "2. D'Agostino Jr, R.B., 1998. Propensity score methods for bias reduction in the comparison of a treatment to a non‐randomized control group. Statistics in medicine, 17(19), pp.2265-2281. https://doi.org/10.1002/(SICI)1097-0258(19981015)17:19%3C2265::AID-SIM918%3E3.0.CO;2-B\n",
    "\n",
    "3. Hirano, K. and Imbens, G.W., 2001. Estimation of causal effects using propensity score weighting: An application to data on right heart catheterization. Health Services and Outcomes research methodology, 2(3), pp.259-278. https://doi.org/10.1023/A:1020371312283\n",
    "\n",
    "4. Austin, P.C., 2011. An introduction to propensity score methods for reducing the effects of confounding in observational studies. Multivariate behavioral research, 46(3), pp.399-424. https://dx.doi.org/10.1080%2F00273171.2011.568786\n",
    "\n",
    "5. Stuart, E.A., 2009. Matching methods for causal inference: A review and a look forward. Baltimore, MD: Johns Hopkins Bloomberg School of Public Health. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2943670/\n",
    "\n",
    "6. Lunceford, J.K., 2017. Stratification and weighting via the propensity score in estimation of causal treatment effects: a comparative study. Statistics in medicine, 36(14), pp.2320-2320. https://doi.org/10.1002/sim.1903\n",
    "\n",
    "7. Overcoming Class Imbalance using SMOTE Techniques, https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/#:~:text=SMOTE%20is%20an%20oversampling%20technique%20where%20the%20synthetic,interpolation%20between%20the%20positive%20instances%20that%20lie%20together.\n",
    "\n",
    "8. Github Page of Applied Data Science https://github.com/TZstatsADS/ADS_Teaching/blob/master/Projects_StarterCodes/Project4-CausalInference/doc/project4_desc.md\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
