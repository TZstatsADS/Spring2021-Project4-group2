{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAT5243 Project 4: Causal Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group 2:**\n",
    "\n",
    "    Wendy Doan\n",
    "    Yibai Liu\n",
    "    Yiwen Fang\n",
    "    Shuqi Yu\n",
    "    Zhihang Xia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***(To all) Things to do:***\n",
    "\n",
    "    1) try other boosting methods (e.g. adaboost) for propensity score estimation\n",
    "    2) time the models\n",
    "    3) finish regression adjustment\n",
    "    4) try reweighting & resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-912e4109760d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# propensity score from tree models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Set a random seed for reproduction.\n",
    "RANDOM_STATE = np.random.seed(2021)\n",
    "\n",
    "# train-test split for propensity score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# propensity score from tree models\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Stratification and Regression Adjustment\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline propensity score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highDim_dataset = pd.read_csv('../data/highDim_dataset.csv')\n",
    "lowDim_dataset = pd.read_csv('../data/lowDim_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_true_ATE = -54.8558\n",
    "low_true_ATE = 2.0901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highDim_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highDim_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowDim_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(highDim_dataset[highDim_dataset.A == 1].Y, 'o', label=\"Treatment\")\n",
    "plt.plot(highDim_dataset[highDim_dataset.A == 0].Y, 'ro', label=\"Control\")\n",
    "plt.title(\"High-dimension data distribution\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lowDim_dataset[lowDim_dataset.A == 1].Y, 'o', label='Treatment')\n",
    "plt.plot(lowDim_dataset[lowDim_dataset.A == 0].Y, 'ro', label = 'Control')\n",
    "plt.title(\"Low-dimension data distribution\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive estimate of ATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_ATE(dataset):\n",
    "    return np.average(dataset[dataset.A == 1].Y) - np.average(dataset[dataset.A == 0].Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# High-dimention\n",
    "naive_ATE(highDim_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Low-dimention\n",
    "naive_ATE(lowDim_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the data sets balanced? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Chloe: go ahead if you want to resample/reweigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"High-dimention\\nTreatment:\", len(highDim_dataset[highDim_dataset.A == 1]), \"\\nControl:\", len(highDim_dataset[highDim_dataset.A == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Low-dimention\\nTreatment:\", len(lowDim_dataset[lowDim_dataset.A == 1]), \"\\nControl:\", len(lowDim_dataset[lowDim_dataset.A == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly imbalanced, but acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test set 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(dataset):\n",
    "    X = np.array(dataset.iloc[:, 2:])\n",
    "    y = np.array(dataset.Y)\n",
    "    A = np.array(dataset.A)\n",
    "    \n",
    "    idx_train, idx_test, y_train, y_test = train_test_split(range(X.shape[0]), y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    X_train = X[idx_train]\n",
    "    X_test = X[idx_test]\n",
    "    A_train = A[idx_train]\n",
    "    A_test = A[idx_test]\n",
    "    return X_train, X_test, y_train, y_test, A_train, A_test\n",
    "\n",
    "# Split arrays into train and test set\n",
    "X_high_train, X_high_test, y_high_train, y_high_test, A_high_train, A_high_test= split_train_test(highDim_dataset)\n",
    "X_low_train, X_low_test, y_low_train, y_low_test, A_low_train, A_low_test= split_train_test(lowDim_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity Score - Logistic Regression (not needed?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High dimensional data baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr_high = LogisticRegression(max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr_high.fit(X_high_train, z_high_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr_high.score(X_high_train, z_high_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logisticRegr_high.score(X_high_test, z_high_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low dimensional data baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr_low = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr_low.fit(X_low_train, z_low_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr_low.score(X_low_train, z_low_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logisticRegr_low.score(X_low_test, z_low_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity Score - Boosted Stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "              'learning_rate': [0.1,0.05,0.01],\n",
    "              'max_depth': [2,3,5],\n",
    "              'min_samples_split': [2, 4],\n",
    "              'n_estimators': [100,150],\n",
    "              'min_samples_leaf':[1,3,5]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X:np.array, A:np.array, model, param_grid=param_grid, cv=10, print_step = True, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Takes a baseline model and does grid search among parameters in the param_grid with cross validation.\n",
    "    Returns the model with best hyparameters after searching\n",
    "    \n",
    "    Inputs:\n",
    "    -------\n",
    "    X: np.array, feature set for fitting the model\n",
    "    y: np.array, target for predictions\n",
    "    model: an sklearn estimator object\n",
    "    param_grid: dict, a dictionary that contains possible search values for parameters\n",
    "    print_step: bool, whether to print detailed scores for each combination of parameters;\n",
    "                set print_step=False to only report the best combination of parameters and the best scores\n",
    "    sample_weight:np.array, instance weights, optional\n",
    "    \n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    The sklearn estimator object with hyperparameters set to be the best parameters from search \n",
    "    \"\"\"\n",
    "    if sample_weight is None:\n",
    "        clf = GridSearchCV(model, param_grid, cv=cv, n_jobs=-1).fit(X, A)\n",
    "    else:\n",
    "        clf = GridSearchCV(model, param_grid, cv=cv, n_jobs=-1).fit(X, A, sample_weight = sample_weight)\n",
    "    print(\"Best accuracy: %0.3f\" % (clf.best_score_))\n",
    "    print()\n",
    "    print(\"Best parameters: %r\" % clf.best_params_)\n",
    "    print('-'*30)\n",
    "    if print_step:\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        \t\tprint(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "        print('-'*30)\n",
    "\n",
    "    return(model.set_params(**clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_high = grid_search(X_high_train, A_high_train, model=GradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "gbm_low = grid_search(X_low_train, A_low_train, model=GradientBoostingClassifier(random_state=RANDOM_STATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A copy of best hyperparameters from grid search\n",
    "# Can skip the last cell because grid search takes minutes\n",
    "best_params_high = {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 4, 'n_estimators': 100}\n",
    "best_params_low = {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 4, 'n_estimators': 100}\n",
    "\n",
    "gbm_high = GradientBoostingClassifier(random_state=RANDOM_STATE).set_params(**best_params_high)\n",
    "gbm_low = GradientBoostingClassifier(random_state=RANDOM_STATE).set_params(**best_params_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "gbm_high = gbm_high.fit(X_high_train, A_high_train)\n",
    "gbm_low = gbm_low.fit(X_low_train, A_low_train)\n",
    "\n",
    "# Predict the propensity scores\n",
    "ps_high_train = gbm_high.predict_proba(X_high_train)[:, 1]\n",
    "ps_low_train = gbm_low.predict_proba(X_low_train)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_high.score(X_high_train, A_high_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_low.score(X_low_train, A_low_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XG Boost (weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_xgb = {\n",
    "    'objective':['binary:logistic', 'reg:squarederror'],\n",
    "    'n_estimators':[10,50,100],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'gamma': [0.5, 1.0, 1.5],\n",
    "    'subsample': [0.3, 0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8],\n",
    "    'max_depth': [3, 4]#,\n",
    "    #'scale_pos_weight':[1, 2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caution: This chunk is sloooooooooow, so maybe you wanna skip this cell and instead use the next \n",
    "# cell which gives the same results\n",
    "xgb_high = grid_search(X_high_train, A_high_train, param_grid = param_grid_xgb, print_step=False, cv=5,\n",
    "                       model=XGBClassifier(n_jobs=-1,random_state=RANDOM_STATE))\n",
    "xgb_low = grid_search(X_low_train, A_low_train, param_grid = param_grid_xgb, print_step=False,cv=5,\n",
    "                      model=XGBClassifier(n_jobs=-1,random_state=RANDOM_STATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A copy of best hyperparameters from grid search\n",
    "# Can skip the last cell because grid search takes minutes\n",
    "best_params_xgb_high = {'colsample_bytree': 0.8, 'gamma': 1.0, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 10, \n",
    "                        'objective': 'reg:squarederror', 'subsample': 0.8}\n",
    "best_params_xgb_low = {'colsample_bytree': 0.6, 'gamma': 1.0, 'max_depth': 3, 'min_child_weight': 10, 'n_estimators': 50, \n",
    "                       'objective': 'binary:logistic', 'subsample': 0.8}\n",
    "\n",
    "xgb_high = XGBClassifier(n_jobs=-1,random_state=RANDOM_STATE).set_params(**best_params_xgb_high)\n",
    "xgb_low = XGBClassifier(n_jobs=-1,random_state=RANDOM_STATE).set_params(**best_params_xgb_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the models\n",
    "xgb_high = xgb_high.fit(X_high_train, A_high_train)\n",
    "xgb_low = xgb_low.fit(X_low_train, A_low_train)\n",
    "\n",
    "# Predict the propensity scores\n",
    "ps_high_train_xgb = xgb_high.predict_proba(X_high_train)[:, 1]\n",
    "ps_low_train_xgb = xgb_low.predict_proba(X_low_train)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_high.score(X_high_train, A_high_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_low.score(X_low_train, A_low_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost with sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_weight_high = np.array([Counter(A_high_train)[0], Counter(A_high_train)[1]])\n",
    "#print(sample_weight_high)\n",
    "\n",
    "#sample_weight_low = np.array([Counter(A_low_train)[0], Counter(A_low_train)[1]])\n",
    "#print(sample_weight_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(ps, A, y):\n",
    "    \"\"\"\n",
    "    Combines propensity scores with A and y\n",
    "    \n",
    "    Returns a data frame with three columns\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame(\n",
    "        np.array([ps, A, y]).T,\n",
    "        columns=['e', 'A', 'Y']\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_high_train = combine_data(ps_high_train, A_high_train, y_high_train)\n",
    "final_low_train = combine_data(ps_low_train, A_low_train, y_low_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_high_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_low_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(final_low_train[final_low_train.A==0].e, \n",
    "         final_low_train[final_low_train.A==0].Y, 'o')\n",
    "plt.plot(final_low_train[final_low_train.A==1].e, \n",
    "         final_low_train[final_low_train.A==1].Y, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATE Estimate - Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stratify(data:pd.DataFrame, k:int):\n",
    "    \n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # rank to resolve duplicate edge cases\n",
    "    data_copy['bin'] = pd.qcut(\n",
    "        data_copy.e.rank(method='first'), k, labels=False\n",
    "    )\n",
    "    \n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stratify(final_high_train, 5).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_ATE_Strat(data:pd.DataFrame, k:int):\n",
    "    \n",
    "    N = data.shape[0]\n",
    "    \n",
    "    data_copy = Stratify(data, k)\n",
    "    \n",
    "    # calculate ATE score\n",
    "    ATE = 0\n",
    "    for j in range(k):\n",
    "        \n",
    "        # temporary data frame\n",
    "        Qj = data_copy[data_copy.bin == j]\n",
    "        Nj = Qj.shape[0]\n",
    "        \n",
    "\n",
    "        s0 = np.average((1- Qj[Qj.A==0].A) * Qj[Qj.A==0].Y)\n",
    "        s1 = np.average(Qj[Qj.A==1].A* Qj[Qj.A==1].Y) if Qj[Qj.A==1].shape[0] != 0 else 0\n",
    "        \n",
    "        ATE += (Nj/N) * (s1 - s0)\n",
    "    \n",
    "    return ATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculate_ATE_Strat(final_high_train, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Calculate_ATE_Strat(final_low_train, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph as functions of bins\n",
    "plt.scatter(range(1,11), [Calculate_ATE_Strat(final_high_train, k) for k in range(1, 11)], label = 'ATE estimate')\n",
    "plt.hlines(high_true_ATE, 1, 10, colors='red', linestyles='dashed', label='ATE true')\n",
    "plt.title(\"High-dimension dataset\\nATE True vs. ATE Estimate by Stratification\")\n",
    "plt.xlabel(\"Number of strata (k)\")\n",
    "plt.ylabel(\"ATE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph as functions of bins\n",
    "plt.scatter(range(1,11), [Calculate_ATE_Strat(final_low_train, k) for k in range(1, 11)], label = 'ATE estimate')\n",
    "plt.hlines(low_true_ATE, 1, 10, colors='red', linestyles='dashed', label='ATE true')\n",
    "plt.title(\"Low-dimension dataset\\nATE True vs. ATE Estimate by Stratification\")\n",
    "plt.xlabel(\"Number of strata (k)\")\n",
    "plt.ylabel(\"ATE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATE Estimate - Regression Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_ATE_RegrAdjusted(data:pd.DataFrame, X_data, k:int):\n",
    "    #model here\n",
    "    y = data.Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATE Estimate - Regression Adjustment with Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wendy to fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question from Chloe: Is this Regression with stratification instead of Regression adjustment with stratification?\n",
    "def Calculate_ATE_StratRegrAdjusted(data:pd.DataFrame, X_data, k:int):\n",
    "    \n",
    "    N = data.shape[0]\n",
    "    \n",
    "    data_adjusted = pd.concat(\n",
    "        [Stratify(data, k), X_data.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    # calculate ATE score\n",
    "    ATE = 0\n",
    "    for j in range(k):\n",
    "        \n",
    "        # temporary data frame\n",
    "        Qj = data_adjusted[data_adjusted.bin == j]\n",
    "        Nj = Qj.shape[0]\n",
    " \n",
    "        # Regression Adjusted Linearly, then Delta_j = alpha^Z_j\n",
    "        X = Qj.drop(['e', 'Y', 'bin'], axis=1)\n",
    "        y = Qj.Y\n",
    "        reg = LinearRegression().fit(X, y)\n",
    "        \n",
    "        ATE += reg.coef_[0]\n",
    "    \n",
    "    return ATE / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculate_ATE_StratRegrAdjusted(final_high_train, pd.DataFrame(X_high_train), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculate_ATE_StratRegrAdjusted(final_low_train, pd.DataFrame(X_low_train), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculate_ATE_StratRegrAdjusted(final_high_train, pd.DataFrame(X_high_train), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Calculate_ATE_StratRegrAdjusted(final_low_train, pd.DataFrame(X_low_train), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph as functions of bins\n",
    "plt.plot([Calculate_ATE_StratRegrAdjusted(final_high_train, pd.DataFrame(X_high_train), k) for k in range(1, 15)], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Chloe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_ATE_StratRegrAdjusted(data:pd.DataFrame, k:int):\n",
    "    \n",
    "    N = data.shape[0]\n",
    "    \n",
    "    data_copy = Stratify(data, k)\n",
    "    \n",
    "    # calculate ATE score\n",
    "    ATE = 0\n",
    "    for j in range(k):\n",
    "        \n",
    "        # stratum j\n",
    "        Qj = data_copy[data_copy.bin == j]\n",
    "        Nj = Qj.shape[0]\n",
    " \n",
    "        X = Qj[['A','e']]\n",
    "        y = Qj.Y\n",
    "        reg = LinearRegression().fit(X, y)\n",
    "        \n",
    "        ATE += reg.coef_[0]\n",
    "    \n",
    "    return ATE / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(1,11), [Calculate_ATE_StratRegrAdjusted(final_high_train, k) for k in range(1, 11)], label = 'ATE estimate')\n",
    "plt.hlines(high_true_ATE, 1, 10, colors='red', linestyles='dashed', label='ATE true')\n",
    "plt.title(\"High-dimension dataset\\nATE True vs. ATE Estimate by Regression Adjustment with Stratification\")\n",
    "plt.xlabel(\"Number of strata (k)\")\n",
    "plt.ylabel(\"ATE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(1,11), [Calculate_ATE_StratRegrAdjusted(final_low_train, k) for k in range(1, 11)], label = 'ATE estimate')\n",
    "plt.hlines(low_true_ATE, 1, 10, colors='red', linestyles='dashed', label='ATE true')\n",
    "plt.title(\"Low-dimension dataset\\nATE True vs. ATE Estimate by Regression Adjustment with Stratification\")\n",
    "plt.xlabel(\"Number of strata (k)\")\n",
    "plt.ylabel(\"ATE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
